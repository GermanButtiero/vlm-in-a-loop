{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download only a subset of the COCO dataset we need to:\n",
    "\n",
    "1- Download the whole dataset\n",
    "\n",
    "2- Create a dataset that select only some categories and it copies the images into a folder \"data\"\n",
    "\n",
    "3- Move the data folder to your HPC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Download the COCO-2017 validation split and load it into FiftyOne\n",
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")\n",
    "\n",
    "# Give the dataset a new name, and make it persistent\n",
    "dataset.name = \"coco-2017-validation\"\n",
    "dataset.persistent = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = foz.load_zoo_dataset(\"coco-2017\", split=\"train\")\n",
    "dataset_train.name = \"coco-2017-train\"\n",
    "dataset_train.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = foz.load_zoo_dataset(\"coco-2017\", split=\"test\")\n",
    "dataset_test.name = \"coco-2017-test\"\n",
    "dataset_test.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2  # OpenCV needed for polygon drawing\n",
    "from pycocotools import mask as coco_mask\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "\n",
    "config = json.load(open(\"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoSegmentationDatasetMRCNN(Dataset):\n",
    "    def __init__(self, image_dir, seg_annotation_file, categories_to_keep=[1], min_area_threshold=655, output_dir=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco_seg = COCO(seg_annotation_file)\n",
    "        self.min_area_threshold = min_area_threshold\n",
    "        self.categories_to_keep = categories_to_keep\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Filter images to keep only those containing objects from specified categories\n",
    "        self.image_ids = []\n",
    "        for cat_id in self.categories_to_keep:\n",
    "            ann_ids = self.coco_seg.getAnnIds(catIds=[cat_id], iscrowd=False)\n",
    "            anns = self.coco_seg.loadAnns(ann_ids)\n",
    "            valid_anns = [ann for ann in anns if ann['area'] >= self.min_area_threshold]\n",
    "            img_ids = list(set([ann['image_id'] for ann in valid_anns]))\n",
    "            self.image_ids.extend(img_ids)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        self.image_ids = list(set(self.image_ids))\n",
    "        print(f\"Dataset contains {len(self.image_ids)} images with categories {categories_to_keep}\")\n",
    "        \n",
    "        # For visualization, create a category mapping\n",
    "        self.category_map = {}\n",
    "        for cat_id in self.categories_to_keep:\n",
    "            cat_info = self.coco_seg.loadCats(cat_id)[0]\n",
    "            self.category_map[cat_id] = cat_info['name']\n",
    "        \n",
    "        # Copy images to output directory if specified\n",
    "        if output_dir:\n",
    "            self.copy_images_to_output_dir()\n",
    "\n",
    "    def copy_images_to_output_dir(self):\n",
    "        \"\"\"Copy filtered images to the output directory\"\"\"\n",
    "        if not self.output_dir:\n",
    "            return\n",
    "            \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Copying {len(self.image_ids)} images to {self.output_dir}...\")\n",
    "        copied_count = 0\n",
    "        \n",
    "        for img_id in self.image_ids:\n",
    "            # Get image info\n",
    "            img_info = self.coco_seg.loadImgs(img_id)[0]\n",
    "            \n",
    "            # Source and destination paths\n",
    "            src_path = os.path.join(self.image_dir, img_info[\"file_name\"])\n",
    "            dst_path = os.path.join(self.output_dir, img_info[\"file_name\"])\n",
    "            \n",
    "            # Copy image if source exists\n",
    "            if os.path.exists(src_path):\n",
    "                os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "                copied_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not find {src_path}\")\n",
    "        \n",
    "        print(f\"Copied {copied_count} images to {self.output_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image_info = self.coco_seg.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Load annotations\n",
    "        ann_ids = self.coco_seg.getAnnIds(imgIds=image_id, catIds=self.categories_to_keep, iscrowd=False)\n",
    "        anns = self.coco_seg.loadAnns(ann_ids)\n",
    "        \n",
    "        # Initialize target dictionary\n",
    "        target = {}\n",
    "        boxes = []\n",
    "        masks = []\n",
    "        labels = []\n",
    "        category_ids = []  # Keep original category IDs for reference\n",
    "        \n",
    "        # Process each annotation\n",
    "        for ann in anns:\n",
    "            if ann['area'] < self.min_area_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Get bounding box\n",
    "            bbox = ann['bbox']  # [x, y, width, height] format\n",
    "            # Convert to [x1, y1, x2, y2] format\n",
    "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "            \n",
    "            # Get mask\n",
    "            mask = self.coco_seg.annToMask(ann)\n",
    "            masks.append(torch.as_tensor(mask, dtype=torch.uint8))\n",
    "            \n",
    "            # Keep original category ID for reference\n",
    "            category_ids.append(ann['category_id'])\n",
    "            \n",
    "            # For segmentation only, use class 1 for all foreground objects\n",
    "            labels.append(1)  # 1 for foreground, 0 for background\n",
    "        \n",
    "        # Convert to tensor format\n",
    "        if boxes:\n",
    "            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target[\"masks\"] = torch.stack(masks)\n",
    "            target[\"category_ids\"] = torch.as_tensor(category_ids, dtype=torch.int64)  # original IDs for reference\n",
    "        else:\n",
    "            # Empty annotations\n",
    "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.zeros((0), dtype=torch.int64)\n",
    "            target[\"masks\"] = torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8)\n",
    "            target[\"category_ids\"] = torch.zeros((0), dtype=torch.int64)\n",
    "        \n",
    "        target[\"image_id\"] = torch.tensor([image_id])\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.75s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset contains 8514 images with categories [5, 23, 53, 33, 13]\n",
      "Copying 8514 images to data/train...\n",
      "Copied 8514 images to data/train\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset contains 330 images with categories [5, 23, 53, 33, 13]\n",
      "Copying 330 images to data/validation...\n",
      "Copied 330 images to data/validation\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CocoSegmentationDatasetMRCNN(\n",
    "    config[\"train_image_dir\"],\n",
    "    config[\"train_annotation_file\"],\n",
    "    categories_to_keep=[5, 23, 53,33, 13], \n",
    "    min_area_threshold=655,\n",
    "    output_dir=\"data/train\"\n",
    ")\n",
    "\n",
    "dataset_val = CocoSegmentationDatasetMRCNN(\n",
    "    config[\"val_image_dir\"],\n",
    "    config[\"val_annotation_file\"],\n",
    "    categories_to_keep=[5, 23, 53, 33,13],\n",
    "    min_area_threshold=655,\n",
    "    output_dir=\"data/validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, image_tensor, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image_tensor.to(device)])[0]\n",
    "    \n",
    "    # Filter predictions based on score\n",
    "    keep = prediction['scores'] > threshold\n",
    "    \n",
    "    filtered_prediction = {\n",
    "        'boxes': prediction['boxes'][keep],\n",
    "        'labels': prediction['labels'][keep],\n",
    "        'scores': prediction['scores'][keep],\n",
    "        'masks': prediction['masks'][keep]\n",
    "    }\n",
    "    \n",
    "    return filtered_prediction\n",
    "\n",
    "def visualize_prediction(image, prediction, dataset):\n",
    "    \"\"\"Visualize model prediction\"\"\"\n",
    "    # Convert image to numpy\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Get components\n",
    "    masks = prediction['masks'].cpu().numpy()\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    labels = prediction['labels'].cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    ax[0].imshow(image_np)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    ax[1].imshow(image_np * 0.5)  # Dim the original image\n",
    "    ax[1].set_title('Predicted Masks')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Generate random colors\n",
    "    n_instances = len(masks)\n",
    "    colors = np.random.rand(n_instances, 3)\n",
    "    \n",
    "    # Draw each mask\n",
    "    for i in range(n_instances):\n",
    "        # Get mask (first channel, threshold at 0.5)\n",
    "        mask = masks[i, 0] > 0.5\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros_like(image_np)\n",
    "        for c in range(3):\n",
    "            colored_mask[:, :, c] = mask * color[c]\n",
    "        \n",
    "        # Overlay on image\n",
    "        mask_area = mask > 0\n",
    "        masked_img = np.where(np.repeat(mask_area[:, :, np.newaxis], 3, axis=2),\n",
    "                            image_np * 0.7 + colored_mask * 0.3,\n",
    "                            ax[1].get_array() * 0.7)\n",
    "        ax[1].imshow(masked_img)\n",
    "        \n",
    "        # Add label info if available\n",
    "        label_id = labels[i]\n",
    "        if hasattr(dataset, 'category_map') and label_id in dataset.category_map:\n",
    "            label_name = dataset.category_map[label_id]\n",
    "        else:\n",
    "            label_name = f\"Class {label_id}\"\n",
    "        \n",
    "        # Find center of mass for text placement\n",
    "        y_indices, x_indices = np.where(mask)\n",
    "        if len(y_indices) > 0 and len(x_indices) > 0:\n",
    "            x_center = int(np.mean(x_indices))\n",
    "            y_center = int(np.mean(y_indices))\n",
    "            \n",
    "            # Add label text\n",
    "            ax[1].text(x_center, y_center, \n",
    "                      f\"{label_name}: {scores[i]:.2f}\",\n",
    "                      color='white', fontsize=8, \n",
    "                      ha='center', va='center',\n",
    "                      bbox=dict(facecolor=color, alpha=0.7, pad=1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test on a validation image\n",
    "with torch.no_grad():\n",
    "    # Get an image from validation set\n",
    "    image, _ = dataset_val[0]\n",
    "    \n",
    "    # Get prediction\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    prediction = get_prediction(model, image, device, threshold=0.7)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_prediction(image, prediction, dataset_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
